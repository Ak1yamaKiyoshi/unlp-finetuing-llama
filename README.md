# unlp-finetuing-llama
 [UNLP shared task](https://github.com/unlp-workshop/unlp-2024-shared-task) 


# Planned: 
chosen model: llama-70b

# Used materials
[llama finetuning](https://medium.com/@csakash03/fine-tuning-llama-2-llm-on-google-colab-a-step-by-step-guide-cf7bb367e790)
[llama RAG (actually needed running model layer by layer to save gpu memory)](https://www.kaggle.com/code/simjeg/platypus2-70b-with-wikipedia-rag/notebook)

# Datasets 
https://huggingface.co/datasets/ukr-detect/ukr-nli-dataset-translated-stanford
https://huggingface.co/datasets/ukr-detect/ukr-formality-dataset-translated-gyafc
https://huggingface.co/datasets/ukr-detect/ukr-toxicity-dataset-translated-jigsaw
https://huggingface.co/datasets/osyvokon/zno
https://huggingface.co/skypro1111/mbart-large-50-verbalization
https://huggingface.co/datasets/skypro1111/ubertext-2-news-verbalized
gradio?
https://huggingface.co/facebook/nllb-200-distilled-600M

